{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77fdd29",
   "metadata": {},
   "source": [
    "# CASA0007 QUANTITATIVE METHODS - COURSEWORK 3\n",
    "\n",
    "Title: How does Urban Greenery Affect Personal Well-Being in London?\n",
    "\n",
    "Abstract: This paper investigates the effect of urban greenery on personal well-being in London and the associated pathway(s) by answering the following research questions:\n",
    "- Does urban greenery affect personal well-being, as measured by ONS4?\n",
    "- What is the pathway(s) by which urban greenery affect personal well-being?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb252646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from statsmodels.tools.tools import add_constant\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import log, sqrt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbf467",
   "metadata": {},
   "source": [
    "## Presentation of Data\n",
    "\n",
    "### Personal Well-Being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2103123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read well-being\n",
    "# Source: https://data.london.gov.uk/dataset/subjective-personal-well-being-borough\n",
    "url = 'https://github.com/ngliangwei15/CASA0007-CW3/blob/main/Data/personal-well-being-borough.xlsx?raw=true'\n",
    "wellbeing = pd.read_excel(url, sheet_name = 'Summary - Mean Scores', skiprows = 1)\n",
    "\n",
    "# Rename and keep useful columns\n",
    "wellbeing_cols = ['Life Satisfaction', 'Worthwhile', 'Happiness']\n",
    "year = '2011/12'\n",
    "wellbeing.rename({year: wellbeing_cols[0],\n",
    "                 year+'.1': wellbeing_cols[1],\n",
    "                 year+'.2': wellbeing_cols[2],\n",
    "                 year+'.3': 'Anxiety'},\n",
    "                axis = 1, inplace = True)\n",
    "wellbeing = wellbeing[['Code', 'Area'] + wellbeing_cols + ['Anxiety']]\n",
    "\n",
    "# Keep only data on London boroughs (Area Code starts with E09)\n",
    "wellbeing.dropna(axis = 0, inplace = True)\n",
    "wellbeing.drop(wellbeing[wellbeing.Area == \"City of London\"].index, inplace = True)\n",
    "wellbeing = wellbeing[wellbeing['Code'].str.match(r'E09')].sort_values(by = ['Code'], ascending=True)\n",
    "wellbeing = wellbeing.reset_index(drop = True)\n",
    "wellbeing.drop('Code', axis = 1, inplace = True)\n",
    "\n",
    "# Set data type\n",
    "wellbeing[wellbeing_cols + ['Anxiety']] = wellbeing[wellbeing_cols + ['Anxiety']].astype('float', copy = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab9c18",
   "metadata": {},
   "source": [
    "### Urban Greenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a026a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read NDVI\n",
    "# Source: https://data.london.gov.uk/dataset/green-and-blue-cover\n",
    "url = 'https://github.com/ngliangwei15/CASA0007-CW3/blob/main/Data/green_cover_borough_summary_0.05.xlsx?raw=true'\n",
    "ndvi_cols = ['lb_code', 'lb_name', 'percent_green']\n",
    "ndvi = pd.read_excel(url, sheet_name = 'borough_green_cover', usecols = ndvi_cols)\n",
    "\n",
    "# Drop City of London\n",
    "ndvi.drop(ndvi[ndvi.lb_code == 'City of London'].index, inplace = True)\n",
    "ndvi.sort_values(by = ['lb_name'], ascending=True)\n",
    "ndvi = ndvi.reset_index(drop = True)\n",
    "\n",
    "# Change name of 'City of Westminster' to Westminster\n",
    "ndvi.loc[ndvi[ndvi.lb_code == 'City of Westminster'].index, 'lb_code'] = 'Westminster'\n",
    "\n",
    "# Rename column\n",
    "ndvi.drop('lb_name', axis = 1, inplace = True)\n",
    "ndvi.rename({'lb_code': 'Area', \n",
    "             'percent_green': 'percentage_greenery'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9d7fe",
   "metadata": {},
   "source": [
    "### Mediators\n",
    "\n",
    "#### Intrinsic well-being enhancement quality of greenery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e58ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anxiety = wellbeing.drop(wellbeing_cols, axis = 1)\n",
    "anxiety.Anxiety = -anxiety.Anxiety\n",
    "wellbeing.drop('Anxiety', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec9702",
   "metadata": {},
   "source": [
    "#### Promotion of physical activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b0bb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in physical exercise\n",
    "# Source: https://data.londonsport.org/dataset/e53dn/physical-activity-levels-by-borough\n",
    "url = 'https://github.com/ngliangwei15/CASA0007-CW3/blob/main/Data/LSD21-APS-Physical-Activity-Levels.xlsx?raw=true'\n",
    "exercise = pd.read_excel(url, sheet_name = '2012', usecols = ['GSS_Code', 'Region', 'Active'])\n",
    "\n",
    "# Keep only data on London boroughs (Area Code starts with E09)\n",
    "exercise = exercise[exercise['GSS_Code'].str.match(r'E09')].sort_values(by = ['GSS_Code'], ascending=True)\n",
    "exercise = exercise.reset_index(drop = True)\n",
    "exercise.drop('GSS_Code', axis = 1, inplace = True)\n",
    "\n",
    "# Convert 'Active' to percentage\n",
    "exercise.Active = exercise.Active*100\n",
    "\n",
    "# Rename columns\n",
    "exercise.rename({'Region': 'Area'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91f5da",
   "metadata": {},
   "source": [
    "#### Enhancement of air quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe21219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in air quality \n",
    "# Source: https://data.london.gov.uk/dataset/llaqm-bespoke-borough-by-borough-air-quality-modelling-and-data\n",
    "url = 'https://github.com/ngliangwei15/CASA0007-CW3/blob/main/Data/NO2_AnnualMean_2013_PopulationData.xlsx?raw=true'\n",
    "air_quality = pd.read_excel(url, sheet_name = 'Borough exceeding 40', skiprows = 4, \n",
    "                            usecols=['Borough Name', 'PopExc2013 %'])\n",
    "air_quality['Borough Name'] = air_quality['Borough Name'].str.replace('&', 'and')\n",
    "\n",
    "# Drop City of London\n",
    "air_quality.drop(air_quality[air_quality['Borough Name'] == 'City of London'].index, inplace = True)\n",
    "air_quality = air_quality.reset_index(drop = True)\n",
    "\n",
    "# Log\n",
    "air_quality['air_quality'] = -air_quality['PopExc2013 %'].apply(log)\n",
    "air_quality.drop('PopExc2013 %', axis = 1, inplace = True)\n",
    "\n",
    "# Rename columns\n",
    "air_quality.rename({'Borough Name': 'Area'},\n",
    "                  axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173f4fb",
   "metadata": {},
   "source": [
    "### Control Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b8f895",
   "metadata": {},
   "source": [
    "#### Self-reported health and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098d31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read self-reported health and age profile \n",
    "# Source: https://data.london.gov.uk/dataset/msoa-atlas\n",
    "url = 'https://raw.githubusercontent.com/ngliangwei15/CASA0007-CW3/main/Data/msoa-data.csv'\n",
    "age_cols = ['Age Structure (2011 Census);0-15;',\n",
    "            'Age Structure (2011 Census);16-29;',\n",
    "            'Age Structure (2011 Census);30-44;',\n",
    "            'Age Structure (2011 Census);45-64;',\n",
    "            'Age Structure (2011 Census);65+;']\n",
    "health_cols = ['Health (2011 Census);Very good health;',\n",
    "              'Health (2011 Census);Good health;',\n",
    "              'Health (2011 Census);Fair health;',\n",
    "              'Health (2011 Census);Bad health;',\n",
    "              'Health (2011 Census);Very bad health;']\n",
    "age_health = pd.read_csv(url,\n",
    "                         usecols = ['Middle Super Output Area', 'MSOA Name',\n",
    "                                    'Age Structure (2011 Census);All Ages;'] + age_cols + health_cols, \n",
    "                         low_memory = False, encoding = 'latin')\n",
    "\n",
    "# Drop row on average for London and City of London\n",
    "age_health.dropna(axis = 0, inplace = True)\n",
    "age_health.drop(age_health[age_health['MSOA Name'] == 'City of London 001'].index, inplace = True)\n",
    "age_health.reset_index(inplace = True)\n",
    "\n",
    "# Change data type\n",
    "age_health[age_cols] = age_health[age_cols].astype('int', copy = False)\n",
    "age_health[health_cols] = age_health[health_cols].astype('int', copy = False)\n",
    "\n",
    "# Drop numbers (last 4 characters) from MSOA name\n",
    "for i in range(0, age_health.shape[0]):\n",
    "               age_health.iloc[i,2] = age_health.iloc[i,2][:-4]\n",
    "\n",
    "# Compute total number of respondents in MSOA\n",
    "respondents = age_health.groupby('MSOA Name').agg({'Age Structure (2011 Census);All Ages;': np.sum}).reset_index()\n",
    "\n",
    "# Compute number and percentage of respondents in each age group\n",
    "age = age_health.groupby('MSOA Name').agg(dict.fromkeys(age_cols, np.sum)).reset_index()\n",
    "age[age_cols] = age[age_cols].div(respondents['Age Structure (2011 Census);All Ages;'], axis = 0) * 100\n",
    "\n",
    "# Compute number and percentage of respondents in each health category\n",
    "health = age_health.groupby('MSOA Name').agg(dict.fromkeys(health_cols, np.sum)).reset_index()\n",
    "health[health_cols] = health[health_cols].div(respondents['Age Structure (2011 Census);All Ages;'], axis = 0) * 100\n",
    "\n",
    "# Rename columns for easy reference\n",
    "age.rename({age_cols[0]: 'Age_0-15',\n",
    "           age_cols[1]: 'Age_15-29',\n",
    "           age_cols[2]: 'Age_30-44',\n",
    "           age_cols[3]: 'Age_45-64',\n",
    "           age_cols[4]: 'Age_65+',\n",
    "           'MSOA Name': 'Area'},\n",
    "          axis = 1, inplace = True)\n",
    "health.rename({health_cols[0]: 'Health_Very_good',\n",
    "               health_cols[1]: 'Health_Good',\n",
    "               health_cols[2]: 'Health_Fair',\n",
    "               health_cols[3]: 'Health_Bad',\n",
    "               health_cols[4]: 'Health_Very_bad',\n",
    "              'MSOA Name': 'Area'},\n",
    "              axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f7434",
   "metadata": {},
   "source": [
    "#### Economic Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a9d89f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(32159140 bytes read, 42311062 more expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1360/2500956248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://github.com/ngliangwei15/CASA0007-CW3/blob/main/Data/economic-inactivity.csv?raw=true'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mecon_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Area'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'percent; Jan 2011-Dec 2011'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0meconomic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mecon_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0meconomic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mecon_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Economic_Inactive'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"method\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         return IOArgs(\n\u001b[1;32m    318\u001b[0m             \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2021/lib/python3.9/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(32159140 bytes read, 42311062 more expected)"
     ]
    }
   ],
   "source": [
    "# Read economic inactivity and rename columns for easy reference\n",
    "# Source: https://data.london.gov.uk/dataset/economic-inactivity-gender\n",
    "url = 'https://github.com/ngliangwei15/CASA0007-CW3/blob/main/Data/economic-inactivity.csv?raw=true'\n",
    "econ_cols = ['Code', 'Area', 'percent; Jan 2011-Dec 2011']\n",
    "economic = pd.read_csv(url, usecols = econ_cols, low_memory = False, encoding = 'latin')\n",
    "economic.rename({econ_cols[2]: 'Economic_Inactive'}, axis = 1, inplace = True)\n",
    "\n",
    "# Keep only data on London boroughs (Area Code starts with E09), drop City of London and change data type\n",
    "economic.dropna(axis = 0, inplace = True)\n",
    "economic = economic[economic['Code'].str.match(r'E09')].sort_values(by = ['Code'], ascending=True)\n",
    "economic.drop(economic[economic.Area == 'City of London'].index, inplace = True)\n",
    "economic = economic.reset_index(drop = True)\n",
    "economic.drop('Code', axis = 1, inplace = True)\n",
    "\n",
    "# Change data type\n",
    "economic.Economic_Inactive = economic.Economic_Inactive.astype('float', copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e69b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read unemployment rate\n",
    "# Source: https://data.london.gov.uk/dataset/employment-occupation-type-and-gender-borough\n",
    "url = 'https://github.com/ngliangwei15/CASA0007-CW3/blob/main/Data/mb-unemployment-rates.xlsx?raw=true'\n",
    "unemployment_cols = ['Code', 'Area', 'Jan 2011-Dec 2011']\n",
    "unemployment = pd.read_excel(url, usecols = unemployment_cols, sheet_name = 'Rates')\n",
    "\n",
    "# Keep only data on London boroughs (Area Code starts with E09), drop City of London and change data type\n",
    "unemployment.dropna(axis = 0, inplace = True)\n",
    "unemployment = unemployment[unemployment['Code'].str.match(r'E09')].sort_values(by = ['Code'], ascending=True)\n",
    "unemployment.drop(unemployment[unemployment.Area == 'City of London'].index, inplace = True)\n",
    "unemployment = unemployment.reset_index(drop = True)\n",
    "\n",
    "# Change data type\n",
    "unemployment['Jan 2011-Dec 2011'] = unemployment['Jan 2011-Dec 2011'].astype('float', copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c01145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute unemployment rate among economically active\n",
    "economic['Economic_Unemployed'] = ((100 - economic.Economic_Inactive)/100) * (unemployment['Jan 2011-Dec 2011']/100) * 100\n",
    "\n",
    "# Compute employment rate among economically active\n",
    "economic['Economic_Employed'] = 100 - economic.Economic_Inactive - economic.Economic_Unemployed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb340e",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of variable names\n",
    "dataset = {'dependent': ['wellbeing'], \n",
    "           'independent': ['ndvi'], \n",
    "           'mediators': ['anxiety', 'exercise', 'air_quality'], \n",
    "           'control': ['health', 'economic', 'age']}\n",
    "\n",
    "# Initialize dataframe\n",
    "data_descriptive = pd.DataFrame(globals()[dataset[[*dataset][0]][0]].Area)\n",
    "\n",
    "# Merge data\n",
    "for value in dataset.values():\n",
    "    for var in value:\n",
    "        data_descriptive = data_descriptive.merge(globals()[var], on = 'Area')\n",
    "        \n",
    "# Descriptive statistics\n",
    "data_descriptive.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943b5b7",
   "metadata": {},
   "source": [
    "## Step 1: Establish the effect of the independent variable on the dependent variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db19961",
   "metadata": {},
   "source": [
    "### Step 1a: Linear regressions of dependent variables on the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d9adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data\n",
    "data = pd.merge(wellbeing, ndvi, on = 'Area')\n",
    "ind_var = ndvi.columns.tolist()[1:]\n",
    "\n",
    "# Perform regression\n",
    "for cat in wellbeing_cols:\n",
    "    print(f'PERFORMING REGRESSION ON:{cat.upper()}')\n",
    "\n",
    "    # Perform regression\n",
    "    reg_model = sm.OLS(endog=data[cat], exog=sm.add_constant(data[ind_var])).fit()\n",
    "    print(reg_model.summary())\n",
    "\n",
    "    # plot residual vs. fit\n",
    "    plt.scatter(reg_model.fittedvalues, reg_model.resid)\n",
    "    plt.xlabel('Fitted Value')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title('Residual vs. Fitted Plot for ' + cat.title())\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b28ea49",
   "metadata": {},
   "source": [
    "Urban greenery had positive effect on all three aspects of well-being, but were significant for LS and WW only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c2e09",
   "metadata": {},
   "source": [
    "### Step 1(b): Control variables were added to the regressions as additional independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data\n",
    "data = pd.merge(wellbeing, ndvi, on = 'Area')\n",
    "data = data.merge(health, on = 'Area')\n",
    "data = data.merge(economic, on = 'Area')\n",
    "data = data.merge(age, on = 'Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a441473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize collinearity\n",
    "df = data.drop(['Area'] + wellbeing.columns.tolist(), axis = 1)\n",
    "plt.figure(figsize=(16, 8))\n",
    "cm = sns.diverging_palette(20, 220, as_cmap=True)\n",
    "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, cmap = cm, annot=True)\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cabf53",
   "metadata": {},
   "source": [
    "#### (i) Using VIF to handle multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9494f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to drop variables using VIF\n",
    "# This function is adjusted from: https://stackoverflow.com/a/51329496/4667568\n",
    "\n",
    "def drop_column_using_vif_(df, thresh=5):\n",
    "    '''\n",
    "    Calculates VIF each feature in a pandas dataframe, and repeatedly drop the columns with the highest VIF\n",
    "    A constant must be added to variance_inflation_factor or the results will be incorrect\n",
    "\n",
    "    :param df: the pandas dataframe containing only the predictor features, not the response variable\n",
    "    :param thresh: (default 5) the threshould VIF value. If the VIF of a variable is greater than thresh, it should be removed from the dataframe\n",
    "    :return: dataframe with multicollinear features removed\n",
    "    '''\n",
    "    while True:\n",
    "        # adding a constant item to the data\n",
    "        df_with_const = add_constant(df)\n",
    "\n",
    "        vif_df = pd.Series([variance_inflation_factor(df_with_const.values, i) \n",
    "               for i in range(df_with_const.shape[1])], name= \"VIF\",\n",
    "              index=df_with_const.columns).to_frame()\n",
    "\n",
    "        # drop the const\n",
    "        vif_df = vif_df.drop('const')\n",
    "        \n",
    "        # if the largest VIF is above the thresh, remove a variable with the largest VIF\n",
    "        # If there are multiple variabels with VIF>thresh, only one of them is removed. This is because we want to keep as many variables as possible\n",
    "        if vif_df.VIF.max() > thresh:\n",
    "            # If there are multiple variables with the maximum VIF, choose the first one\n",
    "            index_to_drop = vif_df.index[vif_df.VIF == vif_df.VIF.max()].tolist()[0]\n",
    "            print('Dropping: {}'.format(index_to_drop))\n",
    "            df = df.drop(columns = index_to_drop)\n",
    "        else:\n",
    "            # No VIF is above threshold. Exit the loop\n",
    "            break\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7945c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop collinear variables\n",
    "ind_var_data = drop_column_using_vif_(df)\n",
    "\n",
    "# Variable names of remaining variables\n",
    "ind_var = ind_var_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression\n",
    "for cat in wellbeing_cols:\n",
    "    print(f'PERFORMING REGRESSION ON:{cat.upper()}')\n",
    "\n",
    "    # Perform regression\n",
    "    reg_model = sm.OLS(endog=data[cat], exog=sm.add_constant(data[ind_var])).fit()\n",
    "    print(reg_model.summary())\n",
    "\n",
    "    # plot residual vs. fit\n",
    "    plt.scatter(reg_model.fittedvalues, reg_model.resid)\n",
    "    plt.xlabel('Fitted Value')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title('Residual vs. Fitted Plot for ' + cat.title())\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459004e2",
   "metadata": {},
   "source": [
    "Urban greenery no longer had a significant effect on well-being, when other variables are controlled for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8876c35",
   "metadata": {},
   "source": [
    "#### (i) Using PCA to handle collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data of independent and control variables\n",
    "data_pca = pd.merge(ndvi, health, on = 'Area')\n",
    "data_pca = data_pca.merge(economic, on = 'Area')\n",
    "data_pca = data_pca.merge(age, on = 'Area')\n",
    "\n",
    "# Standardize data\n",
    "cols = data_pca.columns.tolist()[1:]\n",
    "scalers = [StandardScaler().fit(data_pca[x].values.reshape(-1,1)) for x in cols]\n",
    "data_tr = data_pca.copy()\n",
    "for i in range(0, len(cols)):\n",
    "    data_tr.loc[:,cols[i]] = scalers[i].transform(data_tr[cols[i]].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da078a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=10, whiten=True) \n",
    "pca.fit(data_tr[cols])\n",
    "\n",
    "# Check the variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "singular_values = pca.singular_values_\n",
    "\n",
    "# Plot variance\n",
    "x = np.arange(1,len(explained_variance)+1)\n",
    "plt.plot(x, explained_variance)\n",
    "plt.ylabel('Share of Variance Explained')\n",
    "plt.show()\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(f\"Component {i:>2} accounts for {explained_variance[i]*100:>2.2f}% of variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first four components explain 93.31% of the variance. Hence, we will proceed with 4 components \n",
    "\n",
    "keep_n_components = 4\n",
    "pca = PCA(n_components=keep_n_components, whiten=True)\n",
    "\n",
    "data_tr_pca = pca.fit_transform(data_tr[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back with dataframe\n",
    "\n",
    "for x in [data_tr_pca]:\n",
    "    new_columns = []\n",
    "    \n",
    "    for i in range(0,keep_n_components):\n",
    "        new_columns.append([])\n",
    "\n",
    "    for i in x:\n",
    "        for j in range(0,keep_n_components):\n",
    "            new_columns[j].append(i[j])\n",
    "\n",
    "    for i in range(0,keep_n_components):\n",
    "        data_pca[f\"Component_{i+1}\"] = new_columns[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvectors\n",
    "eig_vec = pca.components_\n",
    "eig_vec_table = pd.DataFrame(cols, columns = ['Indepdendent Variable'])\n",
    "eig_vec_table = eig_vec_table.join(pd.DataFrame(eig_vec).transpose())\n",
    "eig_vec_table = eig_vec_table.rename({0: 'PC1', 1: 'PC2', 2: 'PC3', 3: 'PC4'}, axis = 1)\n",
    "\n",
    "cm = sns.diverging_palette(20, 220, as_cmap=True)\n",
    "x=eig_vec_table.style.background_gradient(cmap=cm)\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7fbbd",
   "metadata": {},
   "source": [
    "Urban greenery was highly loaded in PC1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd186f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "data_pca.drop(['Health_Very_good', 'Health_Good', 'Health_Fair', 'Health_Bad', 'Health_Very_bad', \n",
    "               'Age_0-15', 'Age_15-29', 'Age_30-44', 'Age_45-64', 'Age_65+', \n",
    "               'Economic_Inactive', 'Economic_Unemployed', 'Economic_Employed', \n",
    "               'percentage_greenery'], axis = 1, inplace = True)\n",
    "\n",
    "# Merge data\n",
    "data_reg = wellbeing.merge(data_pca, on = 'Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression\n",
    "for cat in wellbeing_cols:\n",
    "    print(f'PERFORMING REGRESSION OF {cat.upper()}')\n",
    "    \n",
    "    # Perform regression\n",
    "    reg_model = sm.OLS(endog=data_reg[cat], exog=sm.add_constant(data_reg[data_reg.columns.tolist()[4:]])).fit()\n",
    "    #reg_model = back_regress(data_reout[cat], data_reout[group])\n",
    "    print(reg_model.summary())\n",
    "\n",
    "    # plot residual vs. fit\n",
    "    plt.scatter(reg_model.fittedvalues, reg_model.resid)\n",
    "    plt.xlabel('Fitted Value')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title('Residual vs. Fitted Plot for ' + cat.title())\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90263959",
   "metadata": {},
   "source": [
    "PC1, which was negatively related to urban greenery, had a negative relationship with all aspects of well-being, and this was significant for LS and WW, like the observation in step 1(a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b987cfd",
   "metadata": {},
   "source": [
    "## Step 2: Establish the effect of urban greenery on the mediators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframe to track coefficient and standard error\n",
    "path_a = pd.DataFrame({'Mediators': dataset['mediators'], \n",
    "                       'Coeff_a': [0] * len(dataset['mediators']),\n",
    "                       'se_a': [0] * len(dataset['mediators'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get exogeneous variable\n",
    "exog = globals()[dataset['independent'][0]]\n",
    "\n",
    "# Perform regression\n",
    "for mediator in dataset['mediators']:\n",
    "    print(f'PERFORMING REGRESSION OF {mediator.upper()}')\n",
    "\n",
    "    # Get endogeneous variable\n",
    "    endog = globals()[mediator]\n",
    "    \n",
    "    # Merge data\n",
    "    data_reg = pd.merge(endog, exog, on = 'Area')\n",
    "    \n",
    "    # Perform regression\n",
    "    reg_model = sm.OLS(endog= data_reg[endog.columns.tolist()[1:]], \n",
    "                       exog=sm.add_constant(data_reg[exog.columns.tolist()[1:]])).fit()\n",
    "    print(reg_model.summary())\n",
    "\n",
    "    # plot residual vs. fit\n",
    "    plt.scatter(reg_model.fittedvalues, reg_model.resid)\n",
    "    plt.xlabel('Fitted Value')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title('Residual vs. Fitted Plot for ' + cat.title())\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "    \n",
    "    # Store results\n",
    "    path_a.loc[path_a.Mediators == mediator, 'Coeff_a'] = reg_model.params[1]\n",
    "    path_a.loc[path_a.Mediators == mediator, 'se_a'] = reg_model.bse[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d007fb",
   "metadata": {},
   "source": [
    "Urban greenery had a positive and significant effect on air quality, but no significant effect on the other two mediators. Hence, testing for the effect of the mediators on well-being proceeded for enhancement of air quality only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b3757",
   "metadata": {},
   "source": [
    "## Step 3: Establish the effect of the mediators, which were established to be significantly affected by urban greenery, on personal well-being "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb64dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframe to track coefficient and standard error\n",
    "path_b = pd.DataFrame({'Aspect': wellbeing_cols, \n",
    "                       'Coeff_b': [0] * len(wellbeing_cols),\n",
    "                       'se_b': [0] * len(wellbeing_cols)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd53b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data\n",
    "data = pd.merge(wellbeing, ndvi, on = 'Area')\n",
    "data = data.merge(air_quality, on = 'Area')\n",
    "ind_var = data.columns.tolist()[4:]\n",
    "\n",
    "# Perform regression\n",
    "for cat in wellbeing_cols:\n",
    "    print(f'PERFORMING REGRESSION ON:{cat.upper()}')\n",
    "\n",
    "    # Perform regression\n",
    "    reg_model = sm.OLS(endog=data[cat], exog=sm.add_constant(data[ind_var])).fit()\n",
    "    print(reg_model.summary())\n",
    "\n",
    "    # plot residual vs. fit\n",
    "    plt.scatter(reg_model.fittedvalues, reg_model.resid)\n",
    "    plt.xlabel('Fitted Value')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title('Residual vs. Fitted Plot for ' + cat.title())\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "    \n",
    "    # Store results\n",
    "    path_b.loc[path_b.Aspect == cat, 'Coeff_b'] = reg_model.params[2]\n",
    "    path_b.loc[path_b.Aspect == cat, 'se_b'] = reg_model.bse[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8e213",
   "metadata": {},
   "source": [
    "#### Sobel test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "sobel = pd.DataFrame({'Aspect': wellbeing_cols, \n",
    "                       'Coeff_a': path_a.loc[path_a.Mediators == 'air_quality', 'Coeff_a'].tolist() * len(wellbeing_cols),\n",
    "                       'se_a': path_a.loc[path_a.Mediators == 'air_quality', 'se_a'].tolist() * len(wellbeing_cols)})\n",
    "sobel = sobel.merge(path_b, on = 'Aspect')\n",
    "\n",
    "# Compute Z-statistics and p-value\n",
    "sobel['Z-stat'] = (sobel.Coeff_a * sobel.Coeff_b) / (\n",
    "    (((sobel.Coeff_a**2) * (sobel.se_b**2)) + ((sobel.Coeff_b**2) * (sobel.se_a**2))).apply(sqrt))\n",
    "sobel['p-value'] = norm.cdf(sobel['Z-stat'])\n",
    "\n",
    "# Display values\n",
    "sobel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42766821",
   "metadata": {},
   "source": [
    "Enhancement of air quality was a significant mediator between urban greenery, and LS and HP. While urban greenery had a positive relationship on both air quality and well-being, there was a negative relationship between air quality and well-being."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
